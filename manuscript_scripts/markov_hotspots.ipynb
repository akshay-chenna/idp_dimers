{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3067ea81",
   "metadata": {},
   "source": [
    "# Markov Clustering\n",
    "\n",
    "### See hotspot.ipynb, kmeans_modified.ipynb, node_kmeans_modified.ipynb notebooks for trials using other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5027791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from biopandas.pdb import PandasPdb \n",
    "from scipy.spatial import distance\n",
    "import freesasa\n",
    "import markov_clustering as mc\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "!mkdir outputs\n",
    "pdb_path = '../ensembles/PED9AAC-101.pdb'\n",
    "pandas_pdb = PandasPdb().read_pdb(pdb_path)\n",
    "\n",
    "if pandas_pdb.df['ATOM'].chain_id.unique()=='':\n",
    "    pandas_pdb.df['ATOM'].chain_id='A'\n",
    "    pandas_pdb.to_pdb(path=pdb_path[pdb_path.rfind('/')+1:])\n",
    "else:\n",
    "    !cp {pdb_path} .\n",
    "\n",
    "new_path = pdb_path[pdb_path.rfind('/')+1:]\n",
    "pandas_pdb = PandasPdb().read_pdb(new_path)\n",
    "\n",
    "for chains in pandas_pdb.df['ATOM'].chain_id.unique():\n",
    "    \n",
    "    #1. Determine Structural alphabets\n",
    "\n",
    "    coordinates = np.zeros((pandas_pdb.df['ATOM'][pandas_pdb.df['ATOM']['chain_id'] == chains].residue_number.max(),3))\n",
    "    coordinates[:,0] = pandas_pdb.df['ATOM'][(pandas_pdb.df['ATOM']['chain_id'] == chains) & (pandas_pdb.df['ATOM']['atom_name'] == 'CA')].x_coord.to_numpy()\n",
    "    coordinates[:,1] = pandas_pdb.df['ATOM'][(pandas_pdb.df['ATOM']['chain_id'] == chains) & (pandas_pdb.df['ATOM']['atom_name'] == 'CA')].y_coord.to_numpy()\n",
    "    coordinates[:,2] = pandas_pdb.df['ATOM'][(pandas_pdb.df['ATOM']['chain_id'] == chains) & (pandas_pdb.df['ATOM']['atom_name'] == 'CA')].z_coord.to_numpy()\n",
    "\n",
    "    sa_vectors = np.zeros((pandas_pdb.df['ATOM'][pandas_pdb.df['ATOM']['chain_id'] == chains].residue_number.max()-3,4))\n",
    "    sa = np.zeros(len(sa_vectors)) \n",
    "\n",
    "    for residues in range(pandas_pdb.df['ATOM'][pandas_pdb.df['ATOM']['chain_id'] == chains].residue_number.max()-3):\n",
    "        sa_vectors[residues,0] = np.linalg.norm(coordinates[residues+2,:]-coordinates[residues,:])\n",
    "        sa_vectors[residues,1] = np.linalg.norm(coordinates[residues+3,:]-coordinates[residues,:])\n",
    "        sa_vectors[residues,2] = np.linalg.norm(coordinates[residues+3,:]-coordinates[residues+1,:])\n",
    "\n",
    "        v = np.zeros((3,3))\n",
    "        v[0,:] = coordinates[residues+1,:]-coordinates[residues,:]\n",
    "        v[1,:] = coordinates[residues+2,:]-coordinates[residues+1,:]\n",
    "        v[2,:] = coordinates[residues+3,:]-coordinates[residues+2,:]\n",
    "        sa_vectors[residues,3] = np.linalg.det(v)/(np.linalg.norm(v[:,0])*np.linalg.norm(v[:,1]))\n",
    "\n",
    "        sa_definitions = np.loadtxt('sa.in',delimiter=',')\n",
    "\n",
    "        sa_value = np.zeros(len(sa_definitions))\n",
    "        for i in np.arange(len(sa_definitions)):\n",
    "            sa_value[i] = distance.cosine(sa_vectors[residues,:],sa_definitions[i,:])   \n",
    "        sa[residues] = np.argmin(sa_value)+1    \n",
    "        \n",
    "    np.savetxt(\"outputs/\"+new_path[new_path.rfind('/')+1:]+'.'+chains+\".conformer_SA.txt\", sa, fmt='%1d')\n",
    "    \n",
    "    # Structure alphabets 23-27 are beta, remainder have a propensity for beta\n",
    "    sa[sa<=10] = 0\n",
    "    sa[(sa>=15) & (sa<=19)] = 0\n",
    "    sa[sa>0] = 1\n",
    "    \n",
    "    #1.1 Save structural alphabets for sequence identity analysis\n",
    "    sa_save = sa.copy()\n",
    "    sa_save = sa_save - 10\n",
    "    sa_save[sa_save < 0 ] = 0\n",
    "\n",
    "    # 2. Determine surface exposure of structural alphabets\n",
    "    \n",
    "    structure = freesasa.Structure(new_path)\n",
    "    rsa = np.zeros(pandas_pdb.df['ATOM'][pandas_pdb.df['ATOM']['chain_id'] == chains].residue_number.max())\n",
    "    sasa = np.zeros(pandas_pdb.df['ATOM'][pandas_pdb.df['ATOM']['chain_id'] == chains].residue_number.max())\n",
    "    for residues in range(pandas_pdb.df['ATOM'][pandas_pdb.df['ATOM']['chain_id'] == chains].residue_number.max()):\n",
    "        rsa[residues] = freesasa.calc(structure).residueAreas()[chains][str(residues+1)].relativeTotal\n",
    "        sasa[residues] = freesasa.calc(structure).residueAreas()[chains][str(residues+1)].total\n",
    "    exposed_residues = rsa.copy()\n",
    "    exposed_residues[exposed_residues<0.5] = 0\n",
    "    exposed_residues[exposed_residues!=0] = 1\n",
    "    \n",
    "    np.savetxt(\"outputs/\"+new_path[new_path.rfind('/')+1:]+'.'+chains+\".rsa.txt\", rsa, fmt='%.2f')\n",
    "    np.savetxt(\"outputs/\"+new_path[new_path.rfind('/')+1:]+'.'+chains+\".sasa.txt\", sasa, fmt='%.2f')    \n",
    "    \n",
    "    \n",
    "    exposed = np.zeros(len(exposed_residues)-3)\n",
    "    for sas in range(pandas_pdb.df['ATOM'][pandas_pdb.df['ATOM']['chain_id'] == chains].residue_number.max()-3):\n",
    "        exposed[sas] = exposed_residues[sas:sas+4].sum()\n",
    "    exposed[exposed<4] = 0\n",
    "    exposed[exposed!=0] = 1\n",
    "\n",
    "    #3. Determine surface surface SAs that are beta-formed or beta-propensity\n",
    "    \n",
    "    beta_exposed = np.logical_and(sa,exposed)\n",
    "    np.savetxt(\"outputs/\"+new_path[new_path.rfind('/')+1:]+'.'+chains+\".sequence_SA.txt\", sa_save*exposed, fmt='%1d')\n",
    "    \n",
    "    #5. Run voronota to find the CSAs\n",
    "    \n",
    "    !bash voronota.sh {pdb_path[pdb_path.rfind('/')+1:]}\n",
    "    \n",
    "    #6. Load the contact surface area matrix\n",
    " \n",
    "    atom_areas = np.genfromtxt(\"outputs/\"+pdb_path[pdb_path.rfind('/')+1:]+'.atom_contact_areas.txt')\n",
    "\n",
    "    for i in range(len(atom_areas[:,1])):\n",
    "        if np.isnan(atom_areas[i,1]):\n",
    "            atom_areas[i,1] = atom_areas[i,0]\n",
    "\n",
    "    residue_contact_areas = np.zeros((len(atom_areas),3))\n",
    "\n",
    "    residues = np.unique(atom_areas[:,0]).astype(int)\n",
    "    count = 0\n",
    "    for i in residues:\n",
    "        sub_matrix = atom_areas[atom_areas[:,0] == i,:]\n",
    "        contact_residues = np.unique(sub_matrix[:,1])\n",
    "        for c in contact_residues:\n",
    "            residue_contact_areas[count,0] = i\n",
    "            residue_contact_areas[count,1] = c        \n",
    "            residue_contact_areas[count,2] = sub_matrix[np.where(sub_matrix[:,1] == c),2].sum()\n",
    "            count += 1\n",
    "\n",
    "    residue_contact_areas = np.delete(residue_contact_areas,np.arange(count,len(residue_contact_areas)),0)\n",
    "\n",
    "    sa_contact_map = np.zeros([int(residue_contact_areas[:,0].max())-3,int(residue_contact_areas[:,0].max())-3])\n",
    "    for i in residues[:-3]:\n",
    "        sub_matrix = residue_contact_areas[(residue_contact_areas[:,0] == i) | (residue_contact_areas[:,0] == i+1) | (residue_contact_areas[:,0] == i+2) | (residue_contact_areas[:,0] == i+3),:]\n",
    "        contact_residues = np.unique(sub_matrix[:,1]).astype(int)\n",
    "        for c in contact_residues:\n",
    "            if c == 1:\n",
    "                sa_contact_map[i-1,c-1] = sa_contact_map[i-1,c-1] + sub_matrix[sub_matrix[:,1] == c,2].sum()\n",
    "            elif c == 2:\n",
    "                sa_contact_map[i-1,c-1] = sa_contact_map[i-1,c-1] + sub_matrix[sub_matrix[:,1] == c,2].sum()\n",
    "                sa_contact_map[i-1,c-2] = sa_contact_map[i-1,c-2] + sub_matrix[sub_matrix[:,1] == c,2].sum()\n",
    "            elif c == 3:\n",
    "                sa_contact_map[i-1,c-1] = sa_contact_map[i-1,c-1] + sub_matrix[sub_matrix[:,1] == c,2].sum()\n",
    "                sa_contact_map[i-1,c-2] = sa_contact_map[i-1,c-2] + sub_matrix[sub_matrix[:,1] == c,2].sum()\n",
    "                sa_contact_map[i-1,c-3] = sa_contact_map[i-1,c-3] + sub_matrix[sub_matrix[:,1] == c,2].sum()\n",
    "            elif c == max(residues)-2:\n",
    "                sa_contact_map[i-1,c-2] = sa_contact_map[i-1,c-2] + sub_matrix[sub_matrix[:,1] == c,2].sum()\n",
    "                sa_contact_map[i-1,c-3] = sa_contact_map[i-1,c-3] + sub_matrix[sub_matrix[:,1] == c,2].sum()\n",
    "                sa_contact_map[i-1,c-4] = sa_contact_map[i-1,c-4] + sub_matrix[sub_matrix[:,1] == c,2].sum()\n",
    "            elif c == max(residues)-1:\n",
    "                sa_contact_map[i-1,c-3] = sa_contact_map[i-1,c-3] + sub_matrix[sub_matrix[:,1] == c,2].sum()\n",
    "                sa_contact_map[i-1,c-4] = sa_contact_map[i-1,c-4] + sub_matrix[sub_matrix[:,1] == c,2].sum()\n",
    "            elif c == max(residues):\n",
    "                sa_contact_map[i-1,c-4] = sa_contact_map[i-1,c-4] + sub_matrix[sub_matrix[:,1] == c,2].sum()\n",
    "            else:\n",
    "                sa_contact_map[i-1,c-1] = sa_contact_map[i-1,c-1] + sub_matrix[sub_matrix[:,1] == c,2].sum()\n",
    "                sa_contact_map[i-1,c-2] = sa_contact_map[i-1,c-2] + sub_matrix[sub_matrix[:,1] == c,2].sum()\n",
    "                sa_contact_map[i-1,c-3] = sa_contact_map[i-1,c-3] + sub_matrix[sub_matrix[:,1] == c,2].sum()\n",
    "                sa_contact_map[i-1,c-4] = sa_contact_map[i-1,c-4] + sub_matrix[sub_matrix[:,1] == c,2].sum()\n",
    "\n",
    "    sa_contact_map = np.tril(sa_contact_map.T,1) + sa_contact_map\n",
    "    contact_map = sa_contact_map[beta_exposed, :][:,beta_exposed]\n",
    "    \n",
    "    plt.figure(figsize=(15,10)).patch.set_facecolor('white')\n",
    "    sb.heatmap(contact_map, linewidths=1, cmap='RdPu')\n",
    "    \n",
    "    plt.savefig(\"outputs/\"+new_path[new_path.rfind('/')+1:]+'.'+chains+'.map.jpg', dpi=600, format='jpg')\n",
    "    \n",
    "    \n",
    "    #7. Cluster using CSAs (contact surface areas)\n",
    "    sp_matrix = sparse.csr_matrix(contact_map)\n",
    "    \n",
    "    inflation_range = np.arange(1.05,3.05,0.05)\n",
    "    Q = np.empty(inflation_range.shape[0])\n",
    "    i = 0\n",
    "    for inflation in inflation_range:\n",
    "        result = mc.run_mcl(sp_matrix, inflation=inflation)\n",
    "        clusters = mc.get_clusters(result)\n",
    "        Q[i] = mc.modularity(matrix=result, clusters=clusters)\n",
    "        i += 1\n",
    "\n",
    "    inflation = inflation_range[Q.argmax()]\n",
    "    result = mc.run_mcl(sp_matrix, inflation=inflation)\n",
    "    clusters = mc.get_clusters(result)\n",
    "    \n",
    "    # Save cluster member information\n",
    "    cluster_sa = []\n",
    "    with open(\"outputs/\"+new_path[new_path.rfind('/')+1:]+'.'+chains+'.clusters', 'w') as f:\n",
    "        for i in range(len(clusters)):\n",
    "            cluster_sa = np.where(beta_exposed == True)[0][np.array(clusters[i])]\n",
    "            res_sa = np.empty((len(cluster_sa),4))\n",
    "            k = 0\n",
    "            for j in cluster_sa:\n",
    "                res_sa[k,:] = range(j,j+4)\n",
    "                k += 1\n",
    "            to_print = np.unique(res_sa.flatten()).astype(int)\n",
    "            f.write(\"%s\\n\" % to_print)\n",
    "            \n",
    "    \n",
    "    #8. Perform cluster analysis and assign scores\n",
    "    SA = np.genfromtxt(\"outputs/\"+new_path[new_path.rfind('/')+1:]+'.'+chains+\".conformer_SA.txt\")\n",
    "    sasa = np.genfromtxt(\"outputs/\"+new_path[new_path.rfind('/')+1:]+'.'+chains+\".sasa.txt\")\n",
    "\n",
    "    # Find number of clusters            \n",
    "    count = len(clusters)\n",
    "    \n",
    "    # Save the structural alphabets and sasa for each cluster /cluster element\n",
    "    with open(\"outputs/\"+new_path[new_path.rfind('/')+1:]+'.'+chains+'.clusters') as fp:\n",
    "        text = fp.read().replace(\"\\n\",\"\").replace(\"]\",\"]\\n\").split(\"\\n\")\n",
    "        for line in range(count):\n",
    "            cluster = np.fromstring(text[line].strip().strip(\"[]\"), dtype=int, sep=' ')\n",
    "            with open(\"outputs/\"+new_path[new_path.rfind('/')+1:]+'.'+chains+\".cluster_sasa.txt\",'a') as st:\n",
    "                np.savetxt(st,np.sum(sasa[cluster], keepdims=True), fmt=\"%.2f\")\n",
    "            with open(\"outputs/\"+new_path[new_path.rfind('/')+1:]+'.'+chains+\".cluster_SA.txt\",'a') as cs:\n",
    "                cs.write(\"%s\\n\" % SA[cluster[:-3]].astype(int))\n",
    "\n",
    "    # Count the number of structural alphabets\n",
    "    count_SA = np.empty((count,3))\n",
    "    with open(\"outputs/\"+new_path[new_path.rfind('/')+1:]+'.'+chains+\".cluster_SA.txt\") as sc:\n",
    "        i = 0\n",
    "        text = sc.read().replace(\"\\n\",\"\").replace(\"]\",\"]\\n\").split(\"\\n\")\n",
    "        for line in range(count):\n",
    "            cluster_SA = np.fromstring(text[line].strip().strip(\"[]\"), dtype=int, sep=' ')\n",
    "            count_SA[i,0] = len(cluster_SA)\n",
    "            count_SA[i,1] = len(cluster_SA[cluster_SA >= 23])\n",
    "            count_SA[i,2] = len(cluster_SA[cluster_SA < 23])\n",
    "            i += 1\n",
    "        np.savetxt(\"outputs/\"+new_path[new_path.rfind('/')+1:]+'.'+chains+\".count_SA.txt\", count_SA, delimiter=\"\\t\", header=\"Total\\tBeta\\tBeta-propensity\", fmt='%.d')\n",
    "\n",
    "    # Find and save the cluster scores\n",
    "    beta_probability = np.genfromtxt(\"transition_probability.in\", skip_header=1)\n",
    "    cluster_score = np.empty((count,1))\n",
    "    with open(\"outputs/\"+new_path[new_path.rfind('/')+1:]+'.'+chains+\".cluster_SA.txt\") as sc:\n",
    "        i = 0\n",
    "        text = sc.read().replace(\"\\n\",\"\").replace(\"]\",\"]\\n\").split(\"\\n\")\n",
    "        for line in range(count):\n",
    "            cluster_SA = np.fromstring(text[line].strip().strip(\"[]\"), dtype=int, sep=' ')\n",
    "            sasa_SA = np.empty(len(cluster_SA))\n",
    "            for j in range(0,len(cluster_SA)):\n",
    "                sasa_SA[j] = sasa[cluster_SA[j]:cluster_SA[j]+3].sum()\n",
    "            beta_cluster_propensity = beta_probability[cluster_SA,1]\n",
    "            cluster_score[i] = (sasa_SA*beta_cluster_propensity.T).sum()\n",
    "            i += 1\n",
    "    score_matrix = np.vstack((np.arange(1,count+1),cluster_score[:,0]))\n",
    "    np.savetxt(\"outputs/\"+new_path[new_path.rfind('/')+1:]+'.'+chains+\".cluster_score.txt\", score_matrix.T, header=\"\\tScore\", fmt='%d \\t %.3f')     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
